from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import Tongyi# è®°å¾—æ ¹æ®å®é™…å¯¼å…¥Tongyi

# class ParagraphGenerator:
#     def __init__(self, model_name):

#         current_api_key = 'sk-ff2f7a46bc0f412c9ae4915a2829465c'  # ä½ çš„DashScope API Key
#         if not current_api_key:
#             raise ValueError("è¯·è®¾ç½® API Key")

#         # åˆå§‹åŒ– LLM
#         self.llm = Tongyi(
#             model_name=model_name,
#             dashscope_api_key=current_api_key
#         )

#         self.prompt_template = PromptTemplate(
#             input_variables=["chapter_title", "reference_title", "reference_summary", "reference_paragraph", "reference_length"],
#             template="""
# ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†™ä½œåŠ©æ‰‹ï¼Œè¯·ä½ æ ¹æ®ä¸‹åˆ—æ®µè½å†…å®¹å’Œæ‘˜è¦ï¼Œç”Ÿæˆä¸ç« èŠ‚æ ‡é¢˜åŒ¹é…çš„æ–°æ®µè½å†…å®¹ã€‚è¦æ±‚å¦‚ä¸‹ï¼š
# 1. ä¿æŒå†…å®¹å›´ç»•ç« èŠ‚æ ‡é¢˜å±•å¼€ï¼›
# 2. ä½¿ç”¨åŸæ®µè½çš„æ ¸å¿ƒä¿¡æ¯ï¼Œä½†å¯ä»¥æ”¹å†™ã€é‡ç»„æˆ–æ‰©å±•ï¼›
# 3. æ§åˆ¶è¾“å‡ºå­—æ•°ä¸åŸæ®µè½å­—æ•°ç›¸è¿‘ï¼ˆè¯¯å·®ä¸è¶…è¿‡ Â±20%ï¼‰ï¼›
# 4. ä¿æŒè¡¨è¾¾é€šé¡ºã€å­¦æœ¯é£æ ¼ä¸€è‡´ã€‚

# ã€ç« èŠ‚æ ‡é¢˜ã€‘ï¼š{chapter_title}
# ã€åŸæ®µè½æ ‡é¢˜ã€‘ï¼š{reference_title}
# ã€åŸæ®µè½æ‘˜è¦ã€‘ï¼š{reference_summary}
# ã€åŸæ®µè½æ­£æ–‡ã€‘ï¼š{reference_paragraph}
# ã€åŸæ®µè½å­—æ•°ã€‘ï¼š{reference_length}

# è¯·ç”Ÿæˆæ–°çš„æ®µè½å†…å®¹ï¼š
# """
#         )
#         self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)

#     def generate_paragraph(self, chapter_title, reference):
#         try:
#             return self.chain.run({
#                 "chapter_title": chapter_title,
#                 "reference_title": reference["title"],
#                 "reference_summary": reference["summary"],
#                 "reference_paragraph": reference["paragraph"],
#                 "reference_length": reference["length"]
#             }).strip()
#         except Exception as e:
#             return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

#     def generate_all(self, data):
#         for chapter in data:
#             chapter_title = chapter["title"]
#             for ref in chapter["references"]:
#                 ref["generated_paragraph"] = self.generate_paragraph(chapter_title, ref)
#         return data

# import tiktoken
# import openai

# class ParagraphGenerator:
#     def __init__(self, model="gpt-3.5-turbo", max_tokens=3000):
#         self.model = model
#         self.max_tokens = max_tokens
#         self.encoding = tiktoken.encoding_for_model(model)

#     def count_tokens(self, text):
#         if isinstance(text, list):
#             text = " ".join(text)
#         return len(self.encoding.encode(text))

#     def generate_combined_paragraph(self, chapter: dict):
#         """
#         è‡ªåŠ¨æ‹¼æ¥å¹¶ç”Ÿæˆå½“å‰ç« èŠ‚å†…å®¹ï¼ˆæŒ‰ token é™åˆ¶åˆ‡åˆ†ï¼‰
#         """
#         title = chapter["title"]
#         references = chapter["references"]
#         combined_paragraphs = []
#         temp_refs = []
#         token_count = self.count_tokens(str(title))

#         for ref in references:
#             content = ref["paragraph"]
#             if isinstance(content, list):
#                 content = "\n".join(content)
#             ref_tokens = self.count_tokens(content)

#             # å¦‚æœåŠ ä¸Šå½“å‰æ®µè½ä¼šè¶…å‡º token é™åˆ¶ï¼Œåˆ™å¤„ç†å·²æœ‰æ®µè½
#             if token_count + ref_tokens > self.max_tokens:
#                 if temp_refs:
#                     result = self._call_openai(temp_refs, title)
#                     combined_paragraphs.append(result)
#                     temp_refs = []
#                     token_count = self.count_tokens(str(title))
#             temp_refs.append(content)
#             token_count += ref_tokens

#         # æœ€åä¸€ä¸ªåˆ†æ®µæœªå¤„ç†çš„ä¹Ÿè¡¥ä¸Š
#         if temp_refs:
#             result = self._call_openai(temp_refs, title)
#             combined_paragraphs.append(result)

#         return "\n\n".join(combined_paragraphs)

#     def generate_all_chapters(self, chapters: list):
#         """
#         æ‰¹é‡å¤„ç†æ‰€æœ‰ç« èŠ‚å¹¶è¿”å›ç”Ÿæˆç»“æœ
#         """
#         results = []
#         for i, chapter in enumerate(chapters):
#             print(f"ğŸ“˜ æ­£åœ¨å¤„ç†ç¬¬ {i+1} ç« ï¼š{chapter['title']}")
#             try:
#                 combined = self.generate_combined_paragraph(chapter)
#                 results.append({
#                     "title": chapter["title"],
#                     "generated_paragraph": combined
#                 })
#             except Exception as e:
#                 print(f"âŒ é”™è¯¯ï¼šç« èŠ‚ {chapter['title']} å¤„ç†å¤±è´¥ï¼ŒåŸå› ï¼š{e}")
#                 results.append({
#                     "title": chapter["title"],
#                     "generated_paragraph": f"[é”™è¯¯] æ— æ³•ç”Ÿæˆè¯¥ç« èŠ‚å†…å®¹ï¼š{e}"
#                 })
#         return results

#     def _call_openai(self, paragraphs: list, title: list):
#         """
#         å†…éƒ¨æ–¹æ³•ï¼šè°ƒç”¨ OpenAI æ¥å£ç”Ÿæˆæ‘˜è¦å†…å®¹
#         """
#         prompt = (
#             f"ä½ æ˜¯ä¸€åå­¦æœ¯å†™ä½œåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ®µè½å†…å®¹ï¼Œå›´ç»•æ ‡é¢˜ {'-'.join(title)}ï¼Œ"
#             f"ç”Ÿæˆä¸€æ®µæ€»ç»“æ€§æ–‡æœ¬ï¼Œé£æ ¼æ­£å¼ã€ç»“æ„æ¸…æ™°ã€å†…å®¹é€šé¡ºï¼Œæ§åˆ¶ç¯‡å¹…åœ¨å‚è€ƒå†…å®¹çš„é•¿åº¦èŒƒå›´å†…ã€‚\n\n"
#             f"å‚è€ƒå†…å®¹ï¼š\n{''.join(paragraphs)}"
#         )

#         messages = [
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯å†…å®¹æ’°å†™åŠ©æ‰‹"},
#             {"role": "user", "content": prompt}
#         ]

#         response = openai.ChatCompletion.create(
#             model=self.model,
#             messages=messages,
#             temperature=0.4
#         )

#         return response["choices"][0]["message"]["content"].strip()

import os
from langchain_community.llms import Tongyi
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
# import tiktoken # No longer strictly needed for Tongyi unless you have a specific use case

# å‡è®¾ä½ çš„ DashScope API Key å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­æˆ–ç›´æ¥æä¾›
# DASH SCOPE_API_KEY = "sk-ff2f7a46bc0f412c9ae4915a2829465c" # æœ€å¥½ä»ç¯å¢ƒå˜é‡è¯»å–

class ParagraphGenerator:
    def __init__(self, model_name="qwen-turbo", dashscope_api_key='sk-ff2f7a46bc0f412c9ae4915a2829465c'): # é»˜è®¤æ¨¡å‹è®¾ä¸ºqwen-turboæˆ–ä½ é€‰æ‹©çš„
        """
        åˆå§‹åŒ– ParagraphGeneratorã€‚
        :param model_name: Tongyi æ¨¡å‹çš„åç§°ï¼Œä¾‹å¦‚ "qwen-turbo", "qwen-plus", "qwen-max"ã€‚
        :param dashscope_api_key: DashScope API Key. å¦‚æœä¸º None, ä¼šå°è¯•ä»ç¯å¢ƒå˜é‡ 'DASHSCOPE_API_KEY' è¯»å–ã€‚
        """
        self.model_name = model_name
        
        if dashscope_api_key:
            current_api_key = dashscope_api_key
        else:
            current_api_key = os.environ.get("DASHSCOPE_API_KEY")
            # current_api_key = 'sk-ff2f7a46bc0f412c9ae4915a2829465c' # æˆ–è€…ç›´æ¥ç¡¬ç¼–ç ï¼Œä½†ä¸æ¨èç”Ÿäº§ç¯å¢ƒ

        if not current_api_key:
            raise ValueError("è¯·è®¾ç½® DashScope API Key æˆ–é€šè¿‡å‚æ•°ä¼ å…¥ã€‚")

        # åˆå§‹åŒ– LLM
        self.llm = Tongyi(
            model_name=self.model_name,
            dashscope_api_key=current_api_key
            # temperature=0.4 # å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®ï¼Œæˆ–è€…åœ¨è°ƒç”¨æ—¶è®¾ç½®
        )

        # æ³¨æ„ï¼šè¿™ä¸ªPromptTemplateæ˜¯ä¸ºå¤„ç†å•ä¸ªå‚è€ƒæ®µè½è®¾è®¡çš„
        self.prompt_template = PromptTemplate(
            input_variables=["chapter_title", "reference_title", "reference_summary", "reference_paragraph", "reference_length"],
            template="""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†™ä½œåŠ©æ‰‹ï¼Œè¯·ä½ æ ¹æ®ä¸‹åˆ—æ®µè½å†…å®¹å’Œæ‘˜è¦ï¼Œç”Ÿæˆä¸ç« èŠ‚æ ‡é¢˜åŒ¹é…çš„æ–°æ®µè½å†…å®¹ã€‚è¦æ±‚å¦‚ä¸‹ï¼š
1. ä¿æŒå†…å®¹å›´ç»•ç« èŠ‚æ ‡é¢˜å±•å¼€ï¼›
2. ä½¿ç”¨åŸæ®µè½çš„æ ¸å¿ƒä¿¡æ¯ï¼Œä½†å¯ä»¥æ”¹å†™ã€é‡ç»„æˆ–æ‰©å±•ï¼›
3. æ§åˆ¶è¾“å‡ºå­—æ•°ä¸åŸæ®µè½å­—æ•°ç›¸è¿‘ï¼ˆè¯¯å·®ä¸è¶…è¿‡ Â±20%ï¼‰ï¼›
4. ä¿æŒè¡¨è¾¾é€šé¡ºã€å­¦æœ¯é£æ ¼ä¸€è‡´ã€‚

ã€ç« èŠ‚æ ‡é¢˜ã€‘ï¼š{chapter_title}
ã€åŸæ®µè½æ ‡é¢˜ã€‘ï¼š{reference_title}
ã€åŸæ®µè½æ‘˜è¦ã€‘ï¼š{reference_summary}
ã€åŸæ®µè½æ­£æ–‡ã€‘ï¼š{reference_paragraph}
ã€åŸæ®µè½å­—æ•°ã€‘ï¼š{reference_length}

è¯·ç”Ÿæˆæ–°çš„æ®µè½å†…å®¹ï¼š
"""
        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)

    def get_character_count(self, text):
        """
        è®¡ç®—æ–‡æœ¬çš„å­—ç¬¦æ•°ã€‚
        """
        if isinstance(text, list):
            text = "\n".join(text) # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå…ˆåˆå¹¶
        return len(text)

    def generate_paragraph_for_reference(self, chapter_title: str, reference: dict):
        """
        ä¸ºå•ä¸ªå‚è€ƒæ–‡çŒ®ç”Ÿæˆæ–°çš„æ®µè½å†…å®¹ã€‚
        å‡è®¾ reference å­—å…¸åŒ…å« 'title', 'summary', 'paragraph' é”®ã€‚
        """
        reference_title = reference.get("title", "N/A") # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œæä¾›é»˜è®¤å€¼
        reference_summary = reference.get("summary", "N/A") # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œæä¾›é»˜è®¤å€¼
        reference_paragraph = reference.get("paragraph", "")
        
        if isinstance(reference_paragraph, list):
            reference_paragraph = "\n".join(reference_paragraph)

        reference_length = self.get_character_count(reference_paragraph)

        if not reference_paragraph.strip():
            # print(f"    Skipping empty reference paragraph for reference title: {reference_title}")
            return "" # å¦‚æœåŸæ®µè½ä¸ºç©ºï¼Œåˆ™ä¸ç”Ÿæˆ

        inputs = {
            "chapter_title": chapter_title,
            "reference_title": reference_title,
            "reference_summary": reference_summary,
            "reference_paragraph": reference_paragraph,
            "reference_length": reference_length
        }
        
        # print(f"    Invoking LLM for reference: {reference_title} (length: {reference_length})")
        # response = self.chain.invoke(inputs, config={"temperature": 0.4}) # Langchain 0.1.0+
        response = self.chain.run(inputs, temperature=0.4) # æ—§ç‰ˆ langchain
        
        # response = self.chain.invoke(inputs, temperature=0.4) # ç¡®ä¿ä½ çš„Langchainç‰ˆæœ¬æ”¯æŒinvokeå’Œconfig
        # generated_text = response.get('text', '') if isinstance(response, dict) else str(response)
        
        return str(response).strip()


    def generate_chapter_content(self, chapter: dict):
        """
        ä¸ºå½“å‰ç« èŠ‚çš„æ‰€æœ‰å‚è€ƒæ–‡çŒ®ç”Ÿæˆæ–°çš„æ®µè½å†…å®¹ï¼Œå¹¶æ‹¼æ¥å®ƒä»¬ã€‚
        ä¸åŸç‰ˆä¸åŒï¼Œè¿™é‡Œæ˜¯ä¸ºæ¯ä¸ªreferenceè°ƒç”¨ä¸€æ¬¡LLMã€‚
        """
        chapter_main_title = chapter["title"]
        references = chapter.get("references", []) # ä½¿ç”¨ .get ä»¥é˜² "references" ä¸å­˜åœ¨
        
        generated_paragraphs_for_chapter = []

        if not references:
            print(f"    No references found for chapter: {chapter_main_title}")
            return "[ä¿¡æ¯] ç« èŠ‚æ²¡æœ‰å‚è€ƒæ–‡çŒ®å†…å®¹å¯ä¾›ç”Ÿæˆã€‚"

        print(f"  Processing {len(references)} references for chapter '{chapter_main_title}'...")
        for i, ref in enumerate(references):
            # print(f"    Processing reference {i+1}/{len(references)}...")
            # å‡è®¾æ¯ä¸ª ref å­—å…¸æœ‰ "title", "summary", "paragraph"
            # å¦‚æœä½ çš„ 'ref' ç»“æ„ä¸åŒ (ä¾‹å¦‚ 'ref["paragraph"]' æ‰æ˜¯å®é™…å†…å®¹), ä½ éœ€è¦è°ƒæ•´
            # è¿™é‡Œçš„ 'ref' ç»“æ„éœ€è¦ç¬¦åˆ generate_paragraph_for_reference çš„æœŸæœ›
            # ä¾‹å¦‚ï¼Œå¦‚æœ 'ref' å·²ç»æ˜¯ {"title": "ref title", "paragraph": "content", "summary": "sum"}
            # é‚£ä¹ˆç›´æ¥ä¼ é€’ ref å³å¯
            
            # æˆ‘ä»¬éœ€è¦ç¡®ä¿ ref å¯¹è±¡åŒ…å« "title", "summary", "paragraph" é”®
            # åŸå§‹ä»£ç ä¸­ ref["paragraph"] æ˜¯å†…å®¹, æ²¡æœ‰æ˜ç¡®çš„ ref["title"] å’Œ ref["summary"]
            # æˆ‘ä»¬éœ€è¦å‡è®¾æˆ–æ„å»ºå®ƒä»¬
            current_ref_data = {
                "title": ref.get("reference_title", f"Reference {i+1}"), # å°è¯•è·å–ï¼Œå¦åˆ™ç”¨å ä½ç¬¦
                "summary": ref.get("reference_summary", "Not available"), # å°è¯•è·å–ï¼Œå¦åˆ™ç”¨å ä½ç¬¦
                "paragraph": ref.get("paragraph", "") # è¿™æ˜¯åŸå§‹ä»£ç ä¸­çš„å†…å®¹
            }

            try:
                new_paragraph = self.generate_paragraph_for_reference(chapter_main_title, current_ref_data)
                if new_paragraph: # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆä¸”å†…å®¹éç©ºæ—¶æ‰æ·»åŠ 
                    generated_paragraphs_for_chapter.append(new_paragraph)
            except Exception as e:
                error_msg = f"[é”™è¯¯] å¤„ç†ç« èŠ‚ '{chapter_main_title}' ä¸­çš„å‚è€ƒæ–‡çŒ® '{current_ref_data['title']}' å¤±è´¥: {e}"
                print(f"    âŒ {error_msg}")
                generated_paragraphs_for_chapter.append(error_msg)
        
        return "\n\n".join(generated_paragraphs_for_chapter)

    def generate_all_chapters(self, chapters: list):
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰ç« èŠ‚å¹¶è¿”å›ç”Ÿæˆç»“æœã€‚
        """
        results = []
        for i, chapter_data in enumerate(chapters):
            print(f"ğŸ“˜ æ­£åœ¨å¤„ç†ç¬¬ {i+1} ç« ï¼š{chapter_data['title']}")
            try:
                # generate_combined_paragraph å·²è¢« generate_chapter_content æ›¿ä»£
                generated_content = self.generate_chapter_content(chapter_data)
                results.append({
                    "title": chapter_data["title"],
                    "generated_content": generated_content # é”®åä¿®æ”¹ä»¥åæ˜ å†…å®¹
                })
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šç« èŠ‚ {chapter_data['title']} å¤„ç†å¤±è´¥ï¼ŒåŸå› ï¼š{e}")
                results.append({
                    "title": chapter_data["title"],
                    "generated_content": f"[é”™è¯¯] æ— æ³•ç”Ÿæˆè¯¥ç« èŠ‚å†…å®¹ï¼š{e}"
                })
        return results
